---
layout: homepage
---

## About Me
I am currently a Scientist at Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (ASTAR), Singapore. Previously, I obtained my Bachelor‚Äôs Degree from University of Electronic Science and Technology of China (UESTC). After that, I obtained my Master and Ph.D degree from Nanyang Technological University (NTU), Singapore, respectively, under the supervision of Prof. Wang Jianliang, Prof. Wen Changyun and Prof. Lin Guosheng. I also cooperated with Prof. Yuan Junsong.

## Research Interests
My research interests include computer vision, machine learning, time series and federated learning.

## Awards
- **A*STAR Research Highlights**, Agency for Science, Technology and Research \[[Link](https://research.a-star.edu.sg/articles/highlights/seamless-operations-with-machine-health-checks/)\]
- **Good Article Promotion**, IEEE/CAA Journal of Automatica Sinica (2024 IF: 15.3)\[[Link](https://mp.weixin.qq.com/s/Mua13qe4LJqt1AwZ2XHFYQ)\] 
- **Best Paper Award**, The 17th IEEE Conference on Industrial Electronics and Applications (2022)\[[Link](https://www.ieeeiciea.org/2022/)\]
- **1st Place Winner**, 4th CVPR UG2+ Challenge (10k USD prizes): Fully Supervised Action Recognition in the Dark (2021)\[[Link](http://cvpr2022.ug2challenge.org/program21/leaderboard21_t2.html)\]

## Services
I am a reviewer of top-tier conferences and journals including CVPR, ICCV, ECCV, TCSVT, TNNLS, TII, PR, etc.

## News
<div class="news-section">
<ul>
  <li><strong>[Jun. 2024]</strong> One paper is accepted by <strong>IEEE Transactions on Industrial Informatics</strong>.</li> 
  <li><strong>[May. 2024]</strong> One paper is accepted by <strong>IEEE Transactions on Artificial Intelligence</strong>.</li> 
  <li><strong>[Feb. 2024]</strong> Our <a href="https://ieeexplore.ieee.org/abstract/document/10065450">AdaNet</a> published on TII is reported on <a href="https://research.a-star.edu.sg/articles/highlights/seamless-operations-with-machine-health-checks/">A*STAR Research Highlights</a>.</li> 
  <li><strong>[Dec. 2023]</strong> One paper is accepted by <strong>IEEE Transactions on Instrumentation and Measurement</strong>.</li> 
  <li><strong>[Oct. 2023]</strong> One paper is accepted by <strong>IEEE Transactions on Neural Networks and Learning Systems</strong>.</li> 
  <li><strong>[Oct. 2023]</strong> One paper is accepted by <strong>IEEE Transactions on Reliability</strong>.</li> 
  <li><strong>[Mar. 2023]</strong> One paper is accepted by <strong>IEEE Transactions on Industrial Informatics</strong>.</li> 
  <li><strong>[Dec. 2022]</strong> Our paper ‚ÄúMulti-task Self-Supervised Adaptation for Reinforcement Learning‚Äù has won <strong>the best paper award</strong> at The 17th IEEE Conference on Industrial Electronics and Applications 2022ÔºÅ</li> 
  <li><strong>[Dec. 2022]</strong> The source code for our <a href="https://github.com/ruibing-jin/Bi_LSTM_TS">Bi-LSTM based Two-Stream Network for RUL</a> is released!</li> 
  <li><strong>[Nov. 2022]</strong> One paper is accepted by <strong>IEEE Transactions on Circuits and Systems for Video Technology</strong>.</li> 
  <li><strong>[Oct. 2022]</strong> üí•üí•Our <a href="https://ieeexplore.ieee.org/document/9849459">PE-Net</a> receives much attention and is reported by the <a href="https://mp.weixin.qq.com/s/Mua13qe4LJqt1AwZ2XHFYQ">official JAS channel</a> and some famous media, such as <a href="https://techxplore.com/news/2022-10-convolutional-neural-network-framework-life.amp">Tech Xplore</a>, <a href="https://www.eurekalert.org/news-releases/968147">EurekAlert!</a>, and <a href="https://www.prnewswire.com/news-releases/new-study-in-ieeecaa-journal-of-automatica-sinica-describes-convolutional-neural-network-framework-to-predict-remaining-useful-life-in-machines-301654980.html">PR Newswire</a>.</li> 
  <li><strong>[Oct. 2022]</strong> The source code for our <a href="https://github.com/ruibing-jin/PE-Net">PE-Net</a> is released!</li> 
  <li><strong>[Aug. 2022]</strong> One paper is accepted by <strong>IEEE/CAA Journal of Automatica Sinica</strong>.</li> 
  <li><strong>[Apr. 2022]</strong> One paper is accepted by <strong>IEEE Transactions on Instrumentation and Measurement</strong>.</li> 
  <li><strong>[Feb. 2022]</strong> One paper is accepted by <strong>Knowledge-Based Systems</strong></li> 
  <li><strong>[Feb. 2022]</strong> One paper is accepted by <strong>Pattern Recognition</strong></li> 
  <li><strong>[Jun. 2021]</strong> Our team <strong>AStarTrek</strong> achieved the <strong>1st place winner</strong> for <a href="http://cvpr2022.ug2challenge.org/program21/leaderboard21_t2.html">the CVPR 2021 UG2+ Challenge Track 2.1</a>, which is officially repored by ASTAR on <a href="https://www.linkedin.com/feed/update/urn:li:activity:6805305218507657216/">LinkedIn</a>.</li> 
  <li><strong>[Jun. 2021]</strong> One paper is accepted by <strong>Journal of Biophotonics</strong>.</li> 
  <li><strong>[Apr. 2021]</strong> One paper is accepted by <strong>Biomedical Optics Express</strong>.</li> 
  <li><strong>[Oct. 2020]</strong> One paper is accepted by <strong>IEEE Signal Processing Letters</strong>.</li> 
</ul>
</div>

## News
<div class="news-section">
<ul>
  <li><strong>[Jul. 2024]</strong> Three papers are accepted to <a href="https://eccv.ecva.net/">ECCV 2024</a>.</li>  
    <li><strong>[June 2024]</strong> Our work on  <a href="https://arxiv.org/abs/2310.11284">self-supervised 3D scene flow estimation</a> is accepted by TPAMI.</li>
  <li><strong>[June 2024]</strong> üî•We present <a href="https://buaacyw.github.io/mesh-anything/">MeshAnything</a>, a study on high-quality mesh generation with autoregressive transformers.</li>
  <li><strong>[May 2024]</strong> <a href="https://icoz69.github.io/stablellava-official/">StableLLaVA</a> is accepted by ACL 2024.</li>
  <li><strong>[April 2024]</strong> üöÄWe introduce <a href="https://github.com/YvanYin/Metric3D">Metric3D V2</a>, the most capable monocular geometry foundation model for depth and normals estimation. Training codes and demos are available!</li>
  <li><strong>[Mar 2024]</strong> üöÄWe introduce <a href="https://deaddawn.github.io/MovieLLM/">MovieLLM</a>, a long-video understanding multimodal LLM.</li>
  <li><strong>[Feb 2024]</strong> <a href="https://buaacyw.github.io/gaussian-editor/">GaussianEditor</a> and <a href="https://arxiv.org/abs/2311.18651">LL3DA</a> are accepted by CVPR2024.</li>
  <li><strong>[Dec 2023]</strong> üöÄüöÄüöÄWe introduce <a href="https://appagent-official.github.io/">AppAgent</a>, a multimodal agent for operating smartphone apps.</li>
  <li><strong>[Dec 2023]</strong> <a href="https://github.com/buaacyw/IT3D-text-to-3D">IT3D</a> is accepted by AAAI 2024.</li>
  <li><strong>[Dec 2023]</strong> We presented <a href="https://icoz69.github.io/facestudio/">FaceStudio</a>, a powerful identity-preserving image synthesis model.</li>
  <li><strong>[Nov 2023]</strong> We presented <a href="https://shapegpt.github.io/">ShapeGPT</a>, a multimodal LLM for 3D shape generation.</li>
  <li><strong>[Nov 2023]</strong> <a href="https://tingxueronghua.github.io/ChartLlama/">ChartLlama</a> is released! It is a powerful LLM for chart understanding and generation.</li>
  <li><strong>[Nov 2023]</strong> We presented <a href="https://buaacyw.github.io/gaussian-editor/">GaussianEditor</a>, a powerful 3D editing algorithm.</li>
  <li><strong>[Oct 2023]</strong> Pleased to be recognized among <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/6">2023 Top 2% Scientists by Stanford University</a>.</li>
  <li><strong>[Sept 2023]</strong> We presented <a href="https://arxiv.org/abs/2309.09724">Robust Depth</a> for robust geometry-preserving zero-shot depth estimation, which is accepted by ICCV 2023.</li>
  <li><strong>[Aug 2023]</strong> We presented <a href="https://github.com/buaacyw/IT3D-text-to-3D">IT3D</a>, a plug-and-play to improve the results of 3D AIGC models.</li>
  <li><strong>[Aug 2023]</strong> We have released <a href="https://icoz69.github.io/stablellava-official/">StableLLaVA</a>, a clever strategy for collecting datasets to train multimodal LLMs.</li>
  <li><strong>[Jul. 2023]</strong> Our work, <a href="https://arxiv.org/abs/2307.10984">Metric3D</a>, accepted by ICCV 2023, won first place in the <a href="https://jspenmar.github.io/MDEC/">2nd Monocular Depth Estimation Competition at CVPR</a>.</li>
  <li><strong>[Jul. 2023]</strong> Three papers are accepted to ICCV 2023.</li>
  <li><strong>[May 2023]</strong> We have released <a href="https://arxiv.org/abs/2305.19012">StyleAvatar3D</a>, a work for 3D stylized avatar generation.</li>
</ul>
</div>

## Selected Publications
[comment]: <>
<div class="paper">
  <div class="teaser" style="float:left;width:30%;margin: 5px 10px 10px 0;"><img src="images/fedalign.png" height="110" style="box-shadow:2px 2px 6px #888888"/></div>
<p><strong>FedAlign: Federated Model Alignment via Data-Free Knowledge Distillation for Machine Fault Diagnosis</strong>
<br />
Wenjun Sun, Ruqiang Yan*, <strong>Ruibing Jin*</strong>, Rui Zhao, Zhenghua Chen
<br />
<em>IEEE Transactions on Instrumentation and Measurement. <strong><i style="color:#1e90ff">TIM</i></strong>.</em>
<br />
<br />
</p>
</div>

[comment]: <>
<div class="paper">
  <div class="teaser" style="float:left;width:30%;margin: 5px 10px 10px 0;"><img src="images/liteformer.png" height="110" style="box-shadow:2px 2px 6px #888888"/></div>
<p><strong>LiteFormer: A Lightweight and Efficient Transformer for Rotating Machine Fault Diagnosis</strong>
<br />
Wenjun Sun, Ruqiang Yan*, <strong>Ruibing Jin*</strong>, Jiawen Xu, Yuan Yang, Zhenghua Chen
<br />
<em>IEEE Transactions on Reliability.</em>
<br />
<br />
</p>
</div>


[comment]: <>
<div class="paper">
  <div class="teaser" style="float:left;width:30%;margin: 5px 10px 10px 0;"><img src="images/adanet.png" height="110" style="box-shadow:2px 2px 6px #888888"/></div>
<p><strong>An adaptive and dynamical neural network for machine remaining useful life prediction</strong>
<br />
<strong>Ruibing Jin</strong>, Duo Zhou, Min Wu, Xiaoli Li, Zhenghua Chen
<br />
<em>IEEE Transactions on Industrial Informatics. <strong><i style="color:#1e90ff">TII</i></strong>.</em>
<br />
<br />
</p>
</div>

[comment]: <>
<div class="paper">
  <div class="teaser" style="float:left;width:30%;margin: 5px 10px 10px 0;"><img src="images/pe-net.png" height="110" style="box-shadow:2px 2px 6px #888888"/></div>
<p><strong>Position Encoding Based Convolutional Neural Networks for Machine Remaining Useful Life Prediction</strong>
<br />
<strong>Jin Ruibing</strong>, Wu Min, Wu Keyu, Gao Kaizhou, Chen Zhenghua, Li Xiaoli
<br />
<em>IEEE/CAA Journal of Automatica Sinica. <strong><i style="color:#1e90ff">JAS</i></strong>.</em>
</p>
</div>

[comment]: <>
<div class="paper">
  <div class="teaser" style="float:left;width:30%;margin: 5px 10px 10px 0;"><img src="images/ts_blstm.png" height="110" style="box-shadow:2px 2px 6px #888888"/></div>
<p><strong>Bi-LSTM-Based Two-Stream Network for Machine Remaining Useful Life Prediction</strong>
<br />
<strong>Ruibing Jin</strong>, Zhenghua Chen, Keyu Wu, Min Wu, Xiaoli Li, Ruqiang Yan
<br />
<em>IEEE Transactions on Instrumentation and Measurement. <strong><i style="color:#1e90ff">TIM</i></strong>.</em>
</p>
</div>

[comment]: <>
<div class="paper">
  <div class="teaser" style="float:left;width:30%;margin: 5px 10px 10px 0;"><img src="images/opg.png" height="110" style="box-shadow:2px 2px 6px #888888"/></div>
<p><strong>Online Active Proposal Set Generation for Weakly Supervised Object Detection</strong>
<br />
<strong>Ruibing Jin</strong>, Guosheng Lin, Changyun Wen
<br />
<em>Knowledge-Based Systems. <strong><i style="color:#1e90ff">KBS</i></strong>.</em>
<br /> 
<br /> 
</p>
</div>

[comment]: <>
<div class="paper">
  <div class="teaser" style="float:left;width:30%;margin: 5px 10px 10px 0;"><img src="images/ff_net.png" height="110" style="box-shadow:2px 2px 6px #888888"/></div>
<p><strong>Feature flow: In-network feature flow estimation for video object detection</strong>
<br />
<strong>Ruibing Jin</strong>, Guosheng Lin, Changyun Wen, Jianliang Wang, Fayao Liu
<br />
<em>Pattern Recognition. <strong><i style="color:#1e90ff">PR</i></strong>.</em>
</p>
</div>

